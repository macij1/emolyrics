{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b2ac569",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/iao/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# Cell 1 – Imports and configuration\n",
    "# =========================================================\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03b5653a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset: (491173, 39)\n",
      "                                              lyrics  emotion\n",
      "0  Friends told her she was better off at the bot...  sadness\n",
      "1  Well I heard it, playing soft From a drunken b...  sadness\n",
      "2  Oh my god, did I just say that out loud? Shoul...      joy\n",
      "3  Remember when I called you on the telephone? Y...      joy\n",
      "4  Calling me like I got something to say You tho...      joy\n",
      "\n",
      "Number of rows after dropna: 491173\n",
      "\n",
      "Loading cached embeddings from: ../data/spotify_lyrics_embeddings_all-MiniLM-L6-v2.npy\n",
      "\n",
      "Full output embeddings shape: (492569, 384)\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# Cell 2 – Load real lyrics–emotion data & embeddings\n",
    "# =========================================================\n",
    "\n",
    "TEXT_COL = \"lyrics\"          # from clean-emotion.py\n",
    "TARGET_COL = \"emotion\"       # from clean-emotion.py\n",
    "DATA_PATH = \"../data/spotify_emotion_clean.csv\"\n",
    "\n",
    "# Sentence-transformer hyperparameters\n",
    "ST_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "EMBEDDINGS_PATH = f\"../data/spotify_lyrics_embeddings_{ST_MODEL_NAME.split('/')[-1]}.npy\"\n",
    "RECOMPUTE_EMBEDDINGS = False      # set True if you change model or preprocessing\n",
    "SCALE_EMBEDDINGS = True\n",
    "\n",
    "random_seed = 41\n",
    "rng = np.random.default_rng(random_seed)\n",
    "\n",
    "# Imbalance-handling hyperparameters\n",
    "DOWNSAMPLING_MAX_MULTIPLIER = 10.0  # max examples per class = multiplier * minority class count\n",
    "USE_CLASS_WEIGHTS = True\n",
    "\n",
    "# Regularization hyperparameters\n",
    "LABEL_SMOOTHING = 0.1\n",
    "DROPOUT_RATE = 0.0\n",
    "\n",
    "# Read cleaned dataset\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(\"Loaded dataset:\", df.shape)\n",
    "\n",
    "# Keep rows with both lyrics and emotion\n",
    "df = df.dropna(subset=[TEXT_COL, TARGET_COL])\n",
    "df[TEXT_COL] = df[TEXT_COL].astype(str)\n",
    "\n",
    "# Remember original row order so we can align with precomputed embeddings\n",
    "df[\"row_idx\"] = np.arange(len(df), dtype=np.int64)\n",
    "\n",
    "print(df[[TEXT_COL, TARGET_COL]].head())\n",
    "print(\"\\nNumber of rows after dropna:\", len(df))\n",
    "\n",
    "# ---- One-time embedding pass (cached to disk) ----\n",
    "if os.path.exists(EMBEDDINGS_PATH) and not RECOMPUTE_EMBEDDINGS:\n",
    "    print(f\"\\nLoading cached embeddings from: {EMBEDDINGS_PATH}\")\n",
    "    X_embeddings_full = np.load(EMBEDDINGS_PATH, mmap_mode=\"r\")\n",
    "else:\n",
    "    print(\n",
    "        \"\\nNo cached embeddings found (or RECOMPUTE_EMBEDDINGS=True). \"\n",
    "        \"Computing embeddings once and saving them to disk...\"\n",
    "    )\n",
    "    print(f\"Loading sentence-transformer model '{ST_MODEL_NAME}' on device {device}...\")\n",
    "    st_model = SentenceTransformer(ST_MODEL_NAME, device=str(device))\n",
    "    EMBEDDING_DIM = st_model.get_sentence_embedding_dimension()\n",
    "\n",
    "    X_embeddings_full = st_model.encode(\n",
    "        df[TEXT_COL].tolist(),\n",
    "        batch_size=32,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "    ).astype(np.float32)\n",
    "    print(\"Saving embeddings to:\", EMBEDDINGS_PATH)\n",
    "    np.save(EMBEDDINGS_PATH, X_embeddings_full)\n",
    "\n",
    "print(\"\\nFull output embeddings shape:\", X_embeddings_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ad8abe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class distribution (original):\n",
      " emotion\n",
      "joy        188610\n",
      "sadness    156417\n",
      "anger       94929\n",
      "fear        25917\n",
      "love        25300\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Applying moderate downsampling with max_per_class=253000 (multiplier=10.0 * minority=25300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_p/5w1h40jj05l3vm710fcfdtzr0000gq/T/ipykernel_29050/52903287.py:18: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: g.sample(n=min(len(g), max_per_class), random_state=random_seed))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class distribution (after downsampling):\n",
      " emotion\n",
      "joy        188610\n",
      "sadness    156417\n",
      "anger       94929\n",
      "fear        25917\n",
      "love        25300\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Embeddings shape after downsampling: (491173, 384)\n",
      "Train shape: (392938, 384), Test shape: (98235, 384)\n",
      "Embedding dimension: 384\n",
      "Number of classes: 5\n",
      "\n",
      "Class weights (inverse frequency, training split): [1.0348235  3.7902768  0.52083397 3.8827865  0.62803257]\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# Cell 3 – Downsampling, encoding labels, train/test split\n",
    "# =========================================================\n",
    "\n",
    "# Moderate downsampling to reduce training time while keeping some imbalance\n",
    "original_class_counts = df[TARGET_COL].value_counts()\n",
    "print(\"\\nClass distribution (original):\\n\", original_class_counts)\n",
    "\n",
    "min_count = original_class_counts.min()\n",
    "max_per_class = int(DOWNSAMPLING_MAX_MULTIPLIER * min_count)\n",
    "print(\n",
    "    f\"\\nApplying moderate downsampling with max_per_class={max_per_class} \"\n",
    "    f\"(multiplier={DOWNSAMPLING_MAX_MULTIPLIER} * minority={min_count})\"\n",
    ")\n",
    "\n",
    "df = (\n",
    "    df.groupby(TARGET_COL, group_keys=False)\n",
    "      .apply(lambda g: g.sample(n=min(len(g), max_per_class), random_state=random_seed))\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"\\nClass distribution (after downsampling):\\n\", df[TARGET_COL].value_counts())\n",
    "\n",
    "# Align embeddings with the downsampled DataFrame using the saved row indices\n",
    "selected_idx = df[\"row_idx\"].to_numpy()\n",
    "X_embeddings = X_embeddings_full[selected_idx].astype(np.float32)\n",
    "\n",
    "# We no longer need the helper column\n",
    "df = df.drop(columns=[\"row_idx\"])\n",
    "\n",
    "print(\"\\nEmbeddings shape after downsampling:\", X_embeddings.shape)\n",
    "\n",
    "# Encode class labels\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"Emotion_encoded\"] = label_encoder.fit_transform(df[TARGET_COL])\n",
    "\n",
    "y = df[\"Emotion_encoded\"].values.astype(np.int64)\n",
    "X = X_embeddings.astype(np.float32)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=random_seed, stratify=y\n",
    ")\n",
    "\n",
    "# Scale embedding dimensions (optional but often helpful)\n",
    "if SCALE_EMBEDDINGS:\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train).astype(np.float32)\n",
    "    X_test = scaler.transform(X_test).astype(np.float32)\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "n_classes = len(np.unique(y))\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "print(f\"Embedding dimension: {n_features}\")\n",
    "print(f\"Number of classes: {n_classes}\")\n",
    "\n",
    "# Compute inverse-frequency class weights on the training split (for CrossEntropyLoss)\n",
    "class_weights_tensor = None\n",
    "class_weights = None\n",
    "if USE_CLASS_WEIGHTS:\n",
    "    class_sample_counts = np.bincount(y_train, minlength=n_classes)\n",
    "    class_weights = (len(y_train) / (n_classes * class_sample_counts)).astype(np.float32)\n",
    "    class_weights_tensor = torch.from_numpy(class_weights).to(device)\n",
    "    print(\"\\nClass weights (inverse frequency, training split):\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e030c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Cell 4 – Dataset & model definitions\n",
    "# =========================================================\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        # X is a matrix of sentence-transformer embeddings: shape (num_samples, embedding_dim)\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.y = torch.from_numpy(y).long()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class SimpleLinearNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(LinearReLUDropoutLinearNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class LinearReLUDropoutLinearNet(nn.Module):\n",
    "    \"\"\"Linear layer with 256 ReLU neurons, then a linear layer to num_classes.\n",
    "\n",
    "    Input: batch of embedding vectors of shape (batch_size, embedding_dim).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(LinearReLUDropoutLinearNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "if USE_CLASS_WEIGHTS and class_weights_tensor is not None:\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor, label_smoothing=LABEL_SMOOTHING)\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING)\n",
    "\n",
    "chosen_model = LinearReLUDropoutLinearNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51a0a531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Cell 5 – Training & evaluation helpers\n",
    "# =========================================================\n",
    "\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "    return running_loss / len(dataloader.dataset)\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_targets.extend(y_batch.numpy())\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_targets, all_preds, average=\"macro\", zero_division=0\n",
    "    )\n",
    "\n",
    "    accuracy = np.mean(all_preds == all_targets)\n",
    "\n",
    "    return precision, recall, f1, accuracy, all_targets, all_preds\n",
    "\n",
    "\n",
    "def log_confusion_matrix(writer, all_targets, all_preds, phase, epoch, fold=None):\n",
    "    \"\"\"Compute and log a confusion matrix to TensorBoard.\"\"\"\n",
    "\n",
    "    labels = np.arange(len(label_encoder.classes_))\n",
    "    cm = confusion_matrix(all_targets, all_preds, labels=labels)\n",
    "    class_names = label_encoder.classes_\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    im = ax.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "\n",
    "    ax.set(\n",
    "        xticks=np.arange(cm.shape[1]),\n",
    "        yticks=np.arange(cm.shape[0]),\n",
    "        xticklabels=class_names,\n",
    "        yticklabels=class_names,\n",
    "        ylabel=\"True label\",\n",
    "        xlabel=\"Predicted label\",\n",
    "        title=f\"Confusion matrix ({phase})\",\n",
    "    )\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "    fmt = \"d\"\n",
    "    thresh = cm.max() / 2.0 if cm.max() > 0 else 0\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(\n",
    "                j,\n",
    "                i,\n",
    "                format(cm[i, j], fmt),\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "            )\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    tag = f\"ConfusionMatrix/{phase}\"\n",
    "    if fold is not None:\n",
    "        tag += f\"/fold_{fold}\"\n",
    "\n",
    "    writer.add_figure(tag, fig, global_step=epoch)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28e71f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Fold 1/3 ====\n",
      "Epoch 1/15 - Loss: 1.7310\n",
      "Validation: Fold 1 - Precision: 0.2973 | Recall: 0.3132 | F1: 0.2383\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, NUM_EPOCHS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 57\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     writer\u001b[38;5;241m.\u001b[39madd_scalars(\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss/kcv_train\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     60\u001b[0m         {\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfold_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m: loss},\n\u001b[1;32m     61\u001b[0m         epoch,\n\u001b[1;32m     62\u001b[0m     )\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m EPOCHS_BETWEEN_REPORTS \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m epoch \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m epoch \u001b[38;5;241m==\u001b[39m NUM_EPOCHS:\n",
      "Cell \u001b[0;32mIn[7], line 16\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     14\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_batch)\n\u001b[1;32m     15\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 16\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m X_batch\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader\u001b[38;5;241m.\u001b[39mdataset)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/iao/lib/python3.10/site-packages/torch/optim/optimizer.py:517\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    513\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    514\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    515\u001b[0m             )\n\u001b[0;32m--> 517\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    520\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/iao/lib/python3.10/site-packages/torch/optim/optimizer.py:82\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     81\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 82\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/iao/lib/python3.10/site-packages/torch/optim/adam.py:247\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    235\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    237\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    238\u001b[0m         group,\n\u001b[1;32m    239\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    244\u001b[0m         state_steps,\n\u001b[1;32m    245\u001b[0m     )\n\u001b[0;32m--> 247\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdecoupled_weight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/iao/lib/python3.10/site-packages/torch/optim/optimizer.py:150\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/iao/lib/python3.10/site-packages/torch/optim/adam.py:953\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    951\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 953\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/iao/lib/python3.10/site-packages/torch/optim/adam.py:466\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[0m\n\u001b[1;32m    462\u001b[0m         exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(\n\u001b[1;32m    463\u001b[0m             grad, grad, value\u001b[38;5;241m=\u001b[39mcast(\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    464\u001b[0m         )\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m     \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[1;32m    469\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# Cell 6 – K-fold cross-validation with TensorBoard\n",
    "# =========================================================\n",
    "\n",
    "K_FOLDS = 3\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 15\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS_BETWEEN_REPORTS = 2\n",
    "\n",
    "skf = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "fold_results = []\n",
    "\n",
    "# Tensorboard logging\n",
    "log_dir = f\"logs/emotion_cv_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "writer = SummaryWriter(log_dir)\n",
    "hparams = {\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"k_folds\": K_FOLDS,\n",
    "    \"num_epochs\": NUM_EPOCHS,\n",
    "    \"epochs_between_reports\": EPOCHS_BETWEEN_REPORTS,\n",
    "    \"downsampling_max_multiplier\": DOWNSAMPLING_MAX_MULTIPLIER,\n",
    "    \"use_class_weights\": USE_CLASS_WEIGHTS,\n",
    "    \"scale_embeddings\": SCALE_EMBEDDINGS,\n",
    "    \"sentence_transformer_model_name\": ST_MODEL_NAME,\n",
    "    \"neural_net_architecture\": chosen_model.__name__,\n",
    "    \"dropout_rate\": DROPOUT_RATE,\n",
    "    \"label_smoothing\": LABEL_SMOOTHING,\n",
    "}\n",
    "writer.add_text(\n",
    "    \"hparams\",\n",
    "    \"\\n\".join(f\"{k}: {v}\" for k, v in hparams.items()),\n",
    ")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train), 1):\n",
    "    print(f\"\\n==== Fold {fold}/{K_FOLDS} ====\")\n",
    "\n",
    "    X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "    y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "    train_dataset = EmotionDataset(X_tr, y_tr)\n",
    "    val_dataset = EmotionDataset(X_val, y_val)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # Model that consumes sentence-transformer embeddings as input features\n",
    "    model = chosen_model(n_features, n_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    best_val_f1 = -1.0\n",
    "\n",
    "    # Train\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        writer.add_scalars(\n",
    "            \"Loss/kcv_train\",\n",
    "            {f\"fold_{fold}\": loss},\n",
    "            epoch,\n",
    "        )\n",
    "        if epoch % EPOCHS_BETWEEN_REPORTS == 0 or epoch == 1 or epoch == NUM_EPOCHS:\n",
    "            print(f\"Epoch {epoch}/{NUM_EPOCHS} - Loss: {loss:.4f}\")\n",
    "\n",
    "            # Evaluate on training split\n",
    "            precision, recall, f1, accuracy, all_targets, all_preds = evaluate_model(model, train_loader, device)\n",
    "            writer.add_scalars(\"F1_Score/kcv_train\", {f\"fold_{fold}\": f1}, epoch)\n",
    "            writer.add_scalars(\"Accuracy/kcv_train\", {f\"fold_{fold}\": accuracy}, epoch)\n",
    "            writer.add_scalars(\"Precision/kcv_train\", {f\"fold_{fold}\": precision}, epoch)\n",
    "            writer.add_scalars(\"Recall/kcv_train\", {f\"fold_{fold}\": recall}, epoch)\n",
    "            log_confusion_matrix(writer, all_targets, all_preds, phase=\"train\", epoch=epoch, fold=fold)\n",
    "\n",
    "            # Evaluate on validation split\n",
    "            precision, recall, f1, accuracy, all_targets, all_preds = evaluate_model(model, val_loader, device)\n",
    "            writer.add_scalars(\"F1_Score/kcv_validation\", {f\"fold_{fold}\": f1}, epoch)\n",
    "            writer.add_scalars(\"Accuracy/kcv_validation\", {f\"fold_{fold}\": accuracy}, epoch)\n",
    "            writer.add_scalars(\"Precision/kcv_validation\", {f\"fold_{fold}\": precision}, epoch)\n",
    "            writer.add_scalars(\"Recall/kcv_validation\", {f\"fold_{fold}\": recall}, epoch)\n",
    "            print(f\"Validation: Fold {fold} - Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f}\")\n",
    "            log_confusion_matrix(writer, all_targets, all_preds, phase=\"val\", epoch=epoch, fold=fold)\n",
    "\n",
    "            best_val_f1 = max(best_val_f1, f1)\n",
    "\n",
    "    fold_results.append({\n",
    "        \"fold\": fold,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"best_val_f1\": best_val_f1,\n",
    "    })\n",
    "\n",
    "print(\"\\n==== Cross-Validation Summary (Macro-Averaged) ====\")\n",
    "for r in fold_results:\n",
    "    print(\n",
    "        f\"Fold {r['fold']}: \"\n",
    "        f\"Precision={r['precision']:.4f}, \"\n",
    "        f\"Recall={r['recall']:.4f}, \"\n",
    "        f\"F1={r['f1']:.4f}, \"\n",
    "        f\"BestValF1={r['best_val_f1']:.4f}\"\n",
    "    )\n",
    "\n",
    "mean_precision = np.mean([r[\"precision\"] for r in fold_results])\n",
    "mean_recall = np.mean([r[\"recall\"] for r in fold_results])\n",
    "mean_f1 = np.mean([r[\"f1\"] for r in fold_results])\n",
    "mean_best_f1 = np.mean([r[\"best_val_f1\"] for r in fold_results])\n",
    "print(\n",
    "    f\"\\nMean over {K_FOLDS} folds - \"\n",
    "    f\"Precision={mean_precision:.4f}, Recall={mean_recall:.4f}, \"\n",
    "    f\"F1={mean_f1:.4f}, BestValF1={mean_best_f1:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "755a2a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Final Model] Epoch 1/15 - Loss: 1.7365\n",
      "[Final Model] Epoch 5/15 - Loss: 1.7212\n",
      "[Final Model] Epoch 10/15 - Loss: 1.7034\n",
      "[Final Model] Epoch 15/15 - Loss: 1.6799\n",
      "\n",
      "==== Test Set Metrics (Macro-Averaged) ====\n",
      "Precision: 0.2947 | Recall: 0.3137 | F1: 0.2393 | Accuracy: 0.2550\n",
      "\n",
      "==== Detailed Classification Report (Test Set) ====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.32      0.36      0.34     18986\n",
      "        fear       0.08      0.40      0.13      5183\n",
      "         joy       0.52      0.16      0.25     37722\n",
      "        love       0.09      0.39      0.15      5060\n",
      "     sadness       0.46      0.26      0.33     31284\n",
      "\n",
      "    accuracy                           0.26     98235\n",
      "   macro avg       0.29      0.31      0.24     98235\n",
      "weighted avg       0.42      0.26      0.28     98235\n",
      "\n",
      "Saved model checkpoint to models/emotion_classifier_v2.pt\n",
      "Saved scaler to models/scaler.joblib\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# Cell 7 – Final training on full train set + test eval + export\n",
    "# =========================================================\n",
    "\n",
    "train_dataset_full = EmotionDataset(X_train, y_train)\n",
    "test_dataset = EmotionDataset(X_test, y_test)\n",
    "\n",
    "train_loader_full = DataLoader(train_dataset_full, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "final_model = chosen_model(n_features, n_classes).to(device)\n",
    "optimizer = torch.optim.Adam(final_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(1, 15 + 1):\n",
    "    loss = train_one_epoch(final_model, train_loader_full, criterion, optimizer, device)\n",
    "    if epoch % 5 == 0 or epoch == 1:\n",
    "        print(f\"[Final Model] Epoch {epoch}/{NUM_EPOCHS} - Loss: {loss:.4f}\")\n",
    "\n",
    "precision, recall, f1, accuracy, y_true_test, y_pred_test = evaluate_model(final_model, test_loader, device)\n",
    "log_confusion_matrix(writer, all_targets, all_preds, phase=\"test\", epoch=epoch, fold=fold)\n",
    "print(\"\\n==== Test Set Metrics (Macro-Averaged) ====\")\n",
    "print(f\"Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f} | Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\n==== Detailed Classification Report (Test Set) ====\")\n",
    "print(classification_report(\n",
    "    y_true_test,\n",
    "    y_pred_test,\n",
    "    target_names=label_encoder.classes_,\n",
    "    zero_division=0,\n",
    "))\n",
    "\n",
    "# Export model checkpoint (compatible with v2_inference.py)\n",
    "EXPORT_DIR = \"models\"\n",
    "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
    "\n",
    "checkpoint = {\n",
    "    \"model_state_dict\": final_model.state_dict(),\n",
    "    \"input_dim\": n_features,\n",
    "    \"n_classes\": n_classes,\n",
    "    \"label_classes\": label_encoder.classes_,\n",
    "    \"sentence_transformer_name\": ST_MODEL_NAME,\n",
    "    \"scale_embeddings\": SCALE_EMBEDDINGS,\n",
    "    \"use_class_weights\": USE_CLASS_WEIGHTS,\n",
    "    \"class_weights\": class_weights if USE_CLASS_WEIGHTS else None,\n",
    "    \"architecture\": chosen_model.__name__,\n",
    "}\n",
    "\n",
    "ckpt_path = os.path.join(EXPORT_DIR, \"emotion_classifier_v2.pt\")\n",
    "torch.save(checkpoint, ckpt_path)\n",
    "print(\"Saved model checkpoint to\", ckpt_path)\n",
    "\n",
    "# Save scaler\n",
    "if SCALE_EMBEDDINGS:\n",
    "    scaler_path = os.path.join(EXPORT_DIR, \"scaler.joblib\")\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    print(\"Saved scaler to\", scaler_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
