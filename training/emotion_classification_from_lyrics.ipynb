{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Emotion Classification with PyTorch (lyrics embeddings)\n",
        "\n",
        "This notebook builds a small neural network using **PyTorch** to classify a song's **emotion** starting from **clean text lyrics**.\n",
        "\n",
        "We assume we start from a `pandas` DataFrame with at least two columns:\n",
        "\n",
        "- `Lyrics`: preprocessed song lyrics as text\n",
        "- `Emotion`: categorical label (e.g. `\"Happy\"`, `\"Sad\"`, ...)\n",
        "\n",
        "We then call a **mocked sentence transformer encoder** (which you will later replace with a real model) to turn each lyric into a fixed-size embedding vector of a **typical dimensionality for this task** (e.g. 768).\n",
        "\n",
        "The steps are:\n",
        "\n",
        "1. Generate a synthetic dataset of **lyrics → emotion** pairs.\n",
        "2. Pass the lyrics through a **mock sentence transformer** to obtain embeddings.\n",
        "3. Segment the embeddings into **training** and **test** sets.\n",
        "4. Build a small **feedforward neural network** in PyTorch on top of the embeddings.\n",
        "5. Train the network using **k-fold cross-validation** on the training set.\n",
        "6. Evaluate the model using **precision**, **recall**, and **F1-score**.\n",
        "\n",
        "Later, you can plug in your real lyrics dataset and real sentence-transformer encoder while keeping the rest of the pipeline unchanged.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Imports and configuration\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from datetime import datetime\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Device selection\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load or create lyrics–emotion data\n",
        "\n",
        "In this cell we either:\n",
        "- Load your real lyrics dataset into a `pandas` DataFrame with columns `Lyrics` and `Emotion`, **or**\n",
        "- Create a small synthetic dataset of lyrics–emotion pairs for demo purposes.\n",
        "\n",
        "We then call a **mocked sentence transformer encoder** to turn each lyric into a dense embedding of a typical size for this kind of task (e.g. 768). Later, you can replace the synthetic data and the mock encoder with your real dataset and real sentence-transformer model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                              Lyrics Emotion\n",
            "0  tears fear cold fire dance night fear day fear...     joy\n",
            "1  heart day love heart cry rain sun tears party ...     joy\n",
            "2  dance fear dance smile fear cry heart dance ra...   anger\n",
            "3  tears smile hate night rain dance day fear fir...     joy\n",
            "4  smile cry dance tears smile dance day dance he...    fear\n",
            "\n",
            "Embeddings shape: (100000, 768)\n",
            "\n",
            "Class distribution:\n",
            " Emotion\n",
            "sadness     16800\n",
            "surprise    16758\n",
            "love        16688\n",
            "fear        16613\n",
            "joy         16587\n",
            "anger       16554\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# TODO: Replace this with your real lyrics + emotion data loading\n",
        "# For real data you should provide:\n",
        "#   - A DataFrame `df` with at least:\n",
        "#       - \"Lyrics\": cleaned lyrics text\n",
        "#       - \"Emotion\": string label per track\n",
        "#   - An encoder function that maps a list of texts to a 2D array of embeddings.\n",
        "\n",
        "TEXT_COL = \"Lyrics\"\n",
        "TARGET_COL = \"Emotion\"\n",
        "\n",
        "# Typical sentence-transformer embedding size for emotion/semantic tasks\n",
        "EMBEDDING_DIM = 768\n",
        "\n",
        "# Synthetic example data (for demo). Remove when using real data.\n",
        "num_samples = 100000  # keep relatively small so the demo stays fast\n",
        "random_seed = 41\n",
        "rng = np.random.default_rng(random_seed)\n",
        "\n",
        "EMOTION_CLASSES = [\n",
        "    \"anger\", \"surprise\", \"sadness\", \"joy\", \"love\", \"fear\"\n",
        "]\n",
        "\n",
        "# Very small synthetic vocabulary to build fake lyrics\n",
        "MOCK_WORDS = [\n",
        "    \"love\", \"hate\", \"dance\", \"cry\", \"night\", \"day\", \"heart\", \"alone\",\n",
        "    \"party\", \"rain\", \"sun\", \"fire\", \"cold\", \"fear\", \"smile\", \"tears\",\n",
        "]\n",
        "\n",
        "\n",
        "def make_fake_lyric(min_len: int = 5, max_len: int = 30, rng=None) -> str:\n",
        "    \"\"\"Generate a random lyric-like sentence from a tiny mock vocabulary.\"\"\"\n",
        "    if rng is None:\n",
        "        rng = np.random.default_rng()\n",
        "    length = int(rng.integers(min_len, max_len + 1))\n",
        "    words = rng.choice(MOCK_WORDS, size=length)\n",
        "    return \" \".join(words)\n",
        "\n",
        "\n",
        "# Synthetic lyrics + labels\n",
        "lyrics = [make_fake_lyric(rng=rng) for _ in range(num_samples)]\n",
        "emotion_labels = rng.choice(EMOTION_CLASSES, size=num_samples)\n",
        "\n",
        "df = pd.DataFrame({TEXT_COL: lyrics, TARGET_COL: emotion_labels})\n",
        "\n",
        "\n",
        "def mock_sentence_transformer_encode(texts, embedding_dim: int = EMBEDDING_DIM, random_state: int | None = None) -> np.ndarray:\n",
        "    \"\"\"Mocked sentence-transformer encoder.\n",
        "\n",
        "    Replace this implementation with a real sentence-transformer `encode` call.\n",
        "    It should take a sequence of strings and return a 2D float32 array of\n",
        "    shape (num_samples, embedding_dim).\n",
        "    \"\"\"\n",
        "    rng_local = np.random.default_rng(random_state)\n",
        "    return rng_local.normal(size=(len(texts), embedding_dim)).astype(np.float32)\n",
        "\n",
        "\n",
        "# Compute synthetic embeddings from lyrics\n",
        "X_embeddings = mock_sentence_transformer_encode(df[TEXT_COL].tolist(), EMBEDDING_DIM, random_state=random_seed)\n",
        "\n",
        "print(df.head())\n",
        "print(\"\\nEmbeddings shape:\", X_embeddings.shape)\n",
        "print(\"\\nClass distribution:\\n\", df[TARGET_COL].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing and train/test split\n",
        "\n",
        "Here we:\n",
        "\n",
        "- Encode the `Emotion` labels to integer class IDs.\n",
        "- Split the **embeddings** into **train** and **test** sets (hold-out test set).\n",
        "- Optionally standardize the embedding dimensions with `StandardScaler` fitted on the training data only.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train shape: (80000, 768), Test shape: (20000, 768)\n",
            "Embedding dimension: 768\n",
            "Number of classes: 6\n"
          ]
        }
      ],
      "source": [
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "df[\"Emotion_encoded\"] = label_encoder.fit_transform(df[TARGET_COL])\n",
        "\n",
        "y = df[\"Emotion_encoded\"].values.astype(np.int64)\n",
        "X = X_embeddings.astype(np.float32)\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=random_seed, stratify=y\n",
        ")\n",
        "\n",
        "# Scale embedding dimensions (optional but often helpful)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "n_features = X_train.shape[1]\n",
        "n_classes = len(np.unique(y))\n",
        "\n",
        "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
        "print(f\"Embedding dimension: {n_features}\")\n",
        "print(f\"Number of classes: {n_classes}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset and model definitions\n",
        "\n",
        "This section defines:\n",
        "\n",
        "- A `Dataset` wrapper (`EmotionDataset`) for PyTorch.\n",
        "- A small feedforward neural network (`SimpleEmotionNet`) for emotion classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        # X is a matrix of sentence-transformer embeddings: shape (num_samples, embedding_dim)\n",
        "        self.X = torch.from_numpy(X).float()\n",
        "        self.y = torch.from_numpy(y).long()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "\n",
        "class SimpleEmotionNet(nn.Module):\n",
        "    \"\"\"Simple emotion classifier on top of fixed embeddings.\n",
        "\n",
        "    Input: batch of embedding vectors of shape (batch_size, embedding_dim).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(SimpleEmotionNet, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training and evaluation helpers\n",
        "\n",
        "We define helper functions to:\n",
        "\n",
        "- Train the model for one epoch.\n",
        "- Evaluate the model and compute **precision**, **recall**, and **F1-score** (macro-averaged).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for X_batch, y_batch in dataloader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * X_batch.size(0)\n",
        "\n",
        "    return running_loss / len(dataloader.dataset)\n",
        "\n",
        "\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in dataloader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            outputs = model(X_batch)\n",
        "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_targets.extend(y_batch.numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_targets = np.array(all_targets)\n",
        "\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        all_targets, all_preds, average=\"macro\", zero_division=0\n",
        "    )\n",
        "\n",
        "    return precision, recall, f1, all_targets, all_preds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## K-fold cross-validation on training set\n",
        "\n",
        "We perform k-fold cross-validation on the **training** data only, to estimate generalization performance before evaluating once on the held-out test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==== Fold 1/5 ====\n",
            "Epoch 1/20 - Loss: 1.7937\n",
            "Epoch 5/20 - Loss: 1.5950\n",
            "Epoch 10/20 - Loss: 1.3435\n",
            "Epoch 15/20 - Loss: 1.2062\n",
            "Epoch 20/20 - Loss: 1.1150\n",
            "Fold 1 - Precision: 0.1706 | Recall: 0.1711 | F1: 0.1683\n",
            "\n",
            "==== Fold 2/5 ====\n",
            "Epoch 1/20 - Loss: 1.7935\n",
            "Epoch 5/20 - Loss: 1.6020\n",
            "Epoch 10/20 - Loss: 1.3490\n",
            "Epoch 15/20 - Loss: 1.2075\n",
            "Epoch 20/20 - Loss: 1.1144\n",
            "Fold 2 - Precision: 0.1699 | Recall: 0.1702 | F1: 0.1659\n",
            "\n",
            "==== Fold 3/5 ====\n",
            "Epoch 1/20 - Loss: 1.7944\n",
            "Epoch 5/20 - Loss: 1.6028\n",
            "Epoch 10/20 - Loss: 1.3464\n",
            "Epoch 15/20 - Loss: 1.2062\n",
            "Epoch 20/20 - Loss: 1.1150\n",
            "Fold 3 - Precision: 0.1639 | Recall: 0.1635 | F1: 0.1627\n",
            "\n",
            "==== Fold 4/5 ====\n",
            "Epoch 1/20 - Loss: 1.7940\n",
            "Epoch 5/20 - Loss: 1.5979\n",
            "Epoch 10/20 - Loss: 1.3445\n",
            "Epoch 15/20 - Loss: 1.2036\n",
            "Epoch 20/20 - Loss: 1.1060\n",
            "Fold 4 - Precision: 0.1640 | Recall: 0.1636 | F1: 0.1631\n",
            "\n",
            "==== Fold 5/5 ====\n",
            "Epoch 1/20 - Loss: 1.7935\n",
            "Epoch 5/20 - Loss: 1.6039\n",
            "Epoch 10/20 - Loss: 1.3539\n",
            "Epoch 15/20 - Loss: 1.2159\n",
            "Epoch 20/20 - Loss: 1.1246\n",
            "Fold 5 - Precision: 0.1682 | Recall: 0.1685 | F1: 0.1658\n",
            "\n",
            "==== Cross-Validation Summary (Macro-Averaged) ====\n",
            "Fold 1: Precision=0.1706, Recall=0.1711, F1=0.1683\n",
            "Fold 2: Precision=0.1699, Recall=0.1702, F1=0.1659\n",
            "Fold 3: Precision=0.1639, Recall=0.1635, F1=0.1627\n",
            "Fold 4: Precision=0.1640, Recall=0.1636, F1=0.1631\n",
            "Fold 5: Precision=0.1682, Recall=0.1685, F1=0.1658\n",
            "\n",
            "Mean over 5 folds - Precision=0.1673, Recall=0.1674, F1=0.1652\n"
          ]
        }
      ],
      "source": [
        "k_folds = 5\n",
        "batch_size = 32\n",
        "num_epochs = 20\n",
        "learning_rate = 1e-3\n",
        "\n",
        "epochs_between_reports = 5\n",
        "\n",
        "skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "fold_results = []\n",
        "\n",
        "# Tensorboard logging\n",
        "log_dir = f\"runs/emotion_cv_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "writer = SummaryWriter(log_dir)\n",
        "hparams = {\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"batch_size\": batch_size,\n",
        "    \"k_folds\": k_folds,\n",
        "    \"num_epochs\": num_epochs,\n",
        "    \"epochs_between_reports\": epochs_between_reports,\n",
        "}\n",
        "writer.add_text(\n",
        "    \"hparams\",\n",
        "    \"\\n\".join(f\"{k}: {v}\" for k, v in hparams.items()),\n",
        ")\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train), 1):\n",
        "    print(f\"\\n==== Fold {fold}/{k_folds} ====\")\n",
        "\n",
        "    X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
        "    y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
        "\n",
        "    train_dataset = EmotionDataset(X_tr, y_tr)\n",
        "    val_dataset = EmotionDataset(X_val, y_val)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Model that consumes sentence-transformer embeddings as input features\n",
        "    model = SimpleEmotionNet(n_features, n_classes).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Train\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        writer.add_scalar(f\"Loss/kcv_train/fold_{fold}\", loss, epoch)\n",
        "        if epoch % epochs_between_reports == 0 or epoch == 1 or epoch == num_epochs:\n",
        "            print(f\"Epoch {epoch}/{num_epochs} - Loss: {loss:.4f}\")\n",
        "            precision, recall, f1, y_true_val, y_pred_val = evaluate_model(model, val_loader, device)\n",
        "            writer.add_scalar(f\"F1_Score/kcv_validation/fold_{fold}\", f1, epoch)\n",
        "\n",
        "    # Evaluate on validation split\n",
        "    print(f\"Fold {fold} - Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f}\")\n",
        "\n",
        "    fold_results.append({\n",
        "        \"fold\": fold,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1,\n",
        "    })\n",
        "\n",
        "print(\"\\n==== Cross-Validation Summary (Macro-Averaged) ====\")\n",
        "for r in fold_results:\n",
        "    print(\n",
        "        f\"Fold {r['fold']}: \"\n",
        "        f\"Precision={r['precision']:.4f}, \"\n",
        "        f\"Recall={r['recall']:.4f}, \"\n",
        "        f\"F1={r['f1']:.4f}\"\n",
        "    )\n",
        "\n",
        "mean_precision = np.mean([r[\"precision\"] for r in fold_results])\n",
        "mean_recall = np.mean([r[\"recall\"] for r in fold_results])\n",
        "mean_f1 = np.mean([r[\"f1\"] for r in fold_results])\n",
        "print(\n",
        "    f\"\\nMean over {k_folds} folds - \"\n",
        "    f\"Precision={mean_precision:.4f}, Recall={mean_recall:.4f}, F1={mean_f1:.4f}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final training on full training set and evaluation on test set\n",
        "\n",
        "Here we train a fresh model on the **entire training set** and then evaluate once on the held-out **test set**, printing macro-averaged metrics and a detailed classification report.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Final Model] Epoch 1/20 - Loss: 1.7934\n",
            "[Final Model] Epoch 5/20 - Loss: 1.6453\n",
            "[Final Model] Epoch 10/20 - Loss: 1.4431\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(final_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 12\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_full\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m epoch \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Final Model] Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[6], line 12\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     10\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_batch)\n\u001b[1;32m     11\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 12\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m X_batch\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader\u001b[38;5;241m.\u001b[39mdataset)\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/iao/lib/python3.10/site-packages/torch/optim/optimizer.py:517\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    513\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    514\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    515\u001b[0m             )\n\u001b[0;32m--> 517\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    520\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/iao/lib/python3.10/site-packages/torch/optim/optimizer.py:82\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     81\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 82\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/iao/lib/python3.10/site-packages/torch/optim/adam.py:247\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    235\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    237\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    238\u001b[0m         group,\n\u001b[1;32m    239\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    244\u001b[0m         state_steps,\n\u001b[1;32m    245\u001b[0m     )\n\u001b[0;32m--> 247\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdecoupled_weight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/iao/lib/python3.10/site-packages/torch/optim/optimizer.py:150\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/iao/lib/python3.10/site-packages/torch/optim/adam.py:953\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    951\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 953\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/iao/lib/python3.10/site-packages/torch/optim/adam.py:466\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[0m\n\u001b[1;32m    462\u001b[0m         exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(\n\u001b[1;32m    463\u001b[0m             grad, grad, value\u001b[38;5;241m=\u001b[39mcast(\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    464\u001b[0m         )\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m     \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[1;32m    469\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train_dataset_full = EmotionDataset(X_train, y_train)\n",
        "test_dataset = EmotionDataset(X_test, y_test)\n",
        "\n",
        "train_loader_full = DataLoader(train_dataset_full, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "final_model = SimpleEmotionNet(n_features, n_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(final_model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    loss = train_one_epoch(final_model, train_loader_full, criterion, optimizer, device)\n",
        "    if epoch % 5 == 0 or epoch == 1:\n",
        "        print(f\"[Final Model] Epoch {epoch}/{num_epochs} - Loss: {loss:.4f}\")\n",
        "\n",
        "precision, recall, f1, y_true_test, y_pred_test = evaluate_model(final_model, test_loader, device)\n",
        "print(\"\\n==== Test Set Metrics (Macro-Averaged) ====\")\n",
        "print(f\"Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f}\")\n",
        "\n",
        "print(\"\\n==== Detailed Classification Report (Test Set) ====\")\n",
        "print(classification_report(\n",
        "    y_true_test,\n",
        "    y_pred_test,\n",
        "    target_names=label_encoder.classes_,\n",
        "    zero_division=0,\n",
        "))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "iao",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
